"""
ê°„ë‹¨í•œ ì²­í¬ ìƒì„± ìŠ¤í¬ë¦½íŠ¸ (ì„ë² ë”© ì—†ì´)
MCP ì„œë²„ìš© chunks.json ìƒì„±
"""

import json
import re
from pathlib import Path

# ê²½ë¡œ ì„¤ì •
PROCESSED_DIR = Path(r"c:\Users\User\Desktop\n8n_comprehension\3gpp_docs\processed")
OUTPUT_FILE = Path(r"c:\Users\User\Desktop\n8n_comprehension\3gpp_docs\chunks\chunks.json")

def load_documents():
    """processed í´ë”ì—ì„œ í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ"""
    documents = []
    txt_files = list(PROCESSED_DIR.glob("*.txt"))
    
    print(f"Found {len(txt_files)} text file(s)")
    
    for txt_path in txt_files:
        # ë¹ˆ íŒŒì¼ ê±´ë„ˆë›°ê¸°
        if txt_path.stat().st_size == 0:
            print(f"[SKIP] Empty file: {txt_path.name}")
            continue
            
        print(f"Loading: {txt_path.name}")
        with open(txt_path, "r", encoding="utf-8") as f:
            content = f.read()
        
        # ê·œê²©ëª… ì¶”ì¶œ
        spec_name = txt_path.stem
        
        documents.append({
            "content": content,
            "source": spec_name
        })
        print(f"  Size: {len(content):,} characters")
    
    return documents

def chunk_text(text, chunk_size=3000, overlap=200):
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
    chunks = []
    
    # í˜ì´ì§€ ë§ˆì»¤ ì œê±°
    text = re.sub(r'\n--- Page \d+/\d+ ---\n', '\n', text)
    
    # ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„í• 
    paragraphs = re.split(r'\n\n+', text)
    
    current_chunk = ""
    
    for para in paragraphs:
        # ë¹ˆ ë¬¸ë‹¨ ê±´ë„ˆë›°ê¸°
        if not para.strip():
            continue
        
        # ì²­í¬ê°€ ë„ˆë¬´ ì»¤ì§€ë©´ ì €ì¥í•˜ê³  ìƒˆë¡œ ì‹œì‘
        if len(current_chunk) + len(para) > chunk_size and current_chunk:
            chunks.append(current_chunk.strip())
            # ì˜¤ë²„ë©ì„ ìœ„í•´ ë§ˆì§€ë§‰ ë¶€ë¶„ ì¼ë¶€ í¬í•¨
            current_chunk = current_chunk[-overlap:] if len(current_chunk) > overlap else ""
        
        current_chunk += para + "\n\n"
    
    # ë§ˆì§€ë§‰ ì²­í¬
    if current_chunk.strip():
        chunks.append(current_chunk.strip())
    
    return chunks

def create_chunks():
    """ëª¨ë“  ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ê³  JSONìœ¼ë¡œ ì €ì¥"""
    
    print("\n" + "="*60)
    print("Loading documents...")
    print("="*60)
    
    documents = load_documents()
    
    print("\n" + "="*60)
    print("Creating chunks...")
    print("="*60)
    
    all_chunks = []
    
    for doc in documents:
        print(f"\nProcessing: {doc['source']}")
        chunks = chunk_text(doc['content'])
        print(f"  Created {len(chunks)} chunks")
        
        # ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ì €ì¥
        for i, chunk in enumerate(chunks):
            all_chunks.append({
                "text": chunk,
                "spec": doc['source'],
                "chunk_id": f"{doc['source']}_{i}"
            })
    
    # chunks í´ë” ìƒì„±
    OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)
    
    # JSON ì €ì¥
    print("\n" + "="*60)
    print(f"Saving to: {OUTPUT_FILE}")
    print("="*60)
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(all_chunks, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… Total chunks created: {len(all_chunks):,}")
    print(f"ğŸ“ Saved to: {OUTPUT_FILE}")
    print(f"ğŸ“Š File size: {OUTPUT_FILE.stat().st_size / (1024*1024):.1f} MB")

if __name__ == "__main__":
    create_chunks()
